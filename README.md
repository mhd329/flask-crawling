# 개요

<br>

- 기능
  - 완제품 컴퓨터 가격 분석 서비스
- 목적
  - 완제품 컴퓨터를 좀 더 저렴하게 구매할 수 있게 정보를 제공함
- 방법
  - 컴퓨존 > '데크스탑, 서버' 탭을 기준
    - prd_subTxt에서 제품 스펙을 확인
  - 데스크탑 메뉴를 클릭하면 나오는 페이지를 페이지 번호별로 파싱
    - <del>requests, BS4 사용</del>
      - Selenium으로 바꿈
        => 동적 요소를 불러오기 위해서는 웹드라이버를 활용해야 했다.
    - MySQL에 저장
      - field로 Mainboard, CPU, VGA, RAM, SSD(HDD), power등을 구분
- 개발 환경 구성
  - 운영 체제: Windows 10
  - 주 개발 언어: Python
  - 개발 도구: PyCharm
  - Frontend: HTML, CSS, Javascript
  - Backend: Flask
  - Web Server: Nginx
  - CI/CD: Github Actions, Docker, Azure

<br>

# 기술 선정 이유

<br>

- 주 개발 언어
  - NodeJS vs Python: 여러 강력한 crawling library들이 python에 많이 있었다. node에서 비슷한 기능들을 구현하려면 직접 해야 하는 부분들이 있었는데, node숙련도가 python에 비해 떨어지기도 하고 시간도 없어서 python으로 빠르게 개발하기로 하였다.
  - VSCode vs PyCharm: 이번에 PyCharm 리딤코드를 받았는데 사용도 해볼 겸 PyCharm을 선택함. 가장 중요한 점은, 코드 몇 줄 작성하자마자 PEP8 컨벤션을 준수하지 않은 코드를 찾아주던데 이것은 상당히 좋은 기능이었다. 나머지 확장 기능은 vscode와 전체적으로 비슷해서 PyCharm을 선택했다.
  - React vs 기본 3종 세트: 기본적으로 정적 페이지를 제공하다가 찾기 요청이 왔을때만 크롤링을 해서 갱신해주면 된다. 구현이 단순해서 굳이 프론트엔드 라이브러리의 필요를 못 느꼈다.
  - Django vs Flask: 프로젝트의 구성이 단순했다. 회원 관리 기능이나 별도로 보안에 신경쓸 기능들이 많지 않았음. 따라서 기본적으로 많은 편의를 제공해주는 대신 무거운 Django를 굳이 쓸 이유가 없었다. flask 개발 초기에 설정할 것은 많았지만 낭비되는 기능 없이 필요한 것만 가져가는 가벼운 구성을 하고 싶었다.
  - Nginx vs Apache: nginx와 apache 모두 멀티프로세싱에 단일 쓰레딩 방식으로 gil을 우회할 수 있기 때문에 둘 중 더 가벼운 구성을 가진 nginx를 선택했다.
  - TravisCI vs Jenkins vs Github Actions: 셋 다 무료인데, 젠킨스의 경우 서버를 별도로 준비하고 거기서 운영해야 하는데 소규모의 프로젝트에서는 적절한 운영 방식이 아닌것 같았다. 그래서 TravisCI를 사용하려고 했는데, Github Actions이 travis와 별로 기능적인 차이도 없으면서 github에서 기본적으로 제공해주기 때문에 절차적으로 더 단순한 Github Actions을 선택했다.

<br>

# 고민사항

<br>

- DB
  1. <del>컴퓨터 가격 정보는 계속 갱신되어야 할까? 아니면 유저가 요청했을 때만 탐색을 할까?</del>
     - <del>만약 계속 갱신된다면 전체적으로 바뀌는 DB를 어떻게 최적화 할까?</del>
       - <del>DB Replication 적용을 검토</del>
     - 매일 혹은 매주 단위의, 주기적으로 정해진 시간에 한 번의 업데이트만 하면 된다.
       - 한 번에 수행하는 쓰기 작업의 크기가 클 수는 있지만 횟수는 많지 않다.
       - 따라서 사용량이 많은 시간에 쓰기 작업을 피하면 DB에 걸리는 부하가 적기 때문에 굳이 이중화를 할 필요는 없다고 생각했다.
       - 대신 읽기 작업은 원본 DB서버 대신 레디스를 사용하여 속도를 높이기로 했다.
  2. <del>대용량 데이터 처리에 속도가 빠른 MongoDB나 Redis를 사용할지 아니면 무결성 유지와 쿼리 조작에 좋은 RDBMS를 쓸지 고민</del>
     - redis를 메인 DB의 캐시 스토어로 활용해서 읽기 속도를 늘리기로 했다.
     - 데이터의 정합성이 중요한 대신 쓰기 작업은 오직 한번이기 때문에 기본적으로는 Write Through 전략을 채용
- Crawling
  1. <del>파싱을 했으나 목표로 하는 Div 안에 아무 데이터도 없던 현상</del>
     - 동적인 요소들은 정적 페이지가 로드되고 나서 렌더링된다.
     - 따라서 selenium을 사용해서 불러올 수 있었다.
  2. <del>스크롤을 내려야 리스트의 요소들이 추가되는 경우</del>
     - 위와 마찬가지로 selenium으로 해결할 수 있었다.
- 불완전한 데이터
  1. 부품 정보의 누락
     - 베어본이나 브랜드 컴퓨터의 경우 메인보드와 같은 부품의 정보가 없음
     - 아래에 기술할 이미지 분석 기술을 도입해서 해결할 예정이다.
  2. <del>서버 컴퓨터나 개별 부품에 대한 필터링 방법</del>
     - 사실 해결이라기 보다는 타협이라고 하면 타협일 수는 있지만, 요새는 베어본으로 나오는 pc조차도 사무용으로 쓰기에는 전혀 문제없기 때문에 포함하는 방향으로 결정했다.

<br>

# 향후 개선 방향

<br>

- 완제품 판매가와 실제 개별 부품을 모아 직접 조립하는 비용으로부터 마진율을 계산해서 알려주는 기능
- flask를 사용하던 많은 프로젝트들이 fastapi로 옮겨가는 추세이다. 그에 따라 fastapi로 전환 예정
- 페이지의 구조가 바뀌면 선택자의 모습도 바뀔 수 있기 때문에 dfs를 적용하여 기존 페이지의 구조에 의존하지 않게 할 필요가 있다.
- 그 외 개발중인 기능
  - 제품의 광고 이미지 분석을 통해 누락된 스펙 정보를 채워서 제공하기
    1. 제품의 링크를 타고 들어가서 먼저 페이지 소스에 스펙 정보가 있는지 확인
    2. DIV pddt_info_area > DIV recom_L 이 존재하는지 확인한다.
       - 만약 존재한다면 정보를 가져온다.
       - 찾을 수 없는 경우 광고 이미지를 가져온다.
         1. 광고 이미지에서 텍스트를 추출한다.
         2. 얻어진 텍스트 형태의 스펙 정보를 엔터티 단위로 분리
         3. 처음에는 월등한 mecab를 사용하려고 했지만, 기본적으로 엔터티 인식을 하지 않았다.
         4. 그래서 대안으로 Komoran이나 khaiii중 하나를 선택하기로 했다.
         5. https://iostream.tistory.com/144
         6. 위 분석 결과를 바탕으로 komoran을 선택했다.
       - 부품 정보를 추출한다.

<br>
